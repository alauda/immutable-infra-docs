---
weight: 40
---
# Creating Clusters

This document provides comprehensive instructions for creating Kubernetes clusters on the DCS platform using Cluster API. The process involves deploying and configuring multiple Kubernetes resources that work together to provision and manage cluster infrastructure.

## Prerequisites

Before creating clusters, ensure all of the following prerequisites are met:

### 1. Required Plugin Installation
Install the following plugins on the <Term name="product" textCase="capitalize" />'s `global` cluster:
- **Cluster API Provider Kubeadm** - Provides Kubernetes cluster bootstrapping capabilities
- **Cluster API Provider DCS** - Enables DCS infrastructure integration and management

For detailed installation instructions, refer to the [Installation Guide](./install.mdx).

### 2. Virtual Machine Template Preparation
For Kubernetes installation, you must:
- Upload the MicroOS image provided by **<Term name="company" textCase="capitalize" />** to the DCS platform
- Create a virtual machine template based on this image
- Ensure the template includes all necessary Kubernetes components

### 3. Network Connectivity
Ensure that all nodes in the <Term name="product" textCase="capitalize" />'s `global` cluster can access the DCS platform on:
- **Port 7443** (DCS API)
- **Port 8443** (DCS Web Console)

> **Requirement:** Connectivity to **both ports** is mandatory for cluster creation and management.

### 4. DCS Platform Credentials
The DCS platform credentials used for cluster creation must meet specific requirements.

**Required User Configuration**
- **User Type**: Must be `Interface interconnection user` (接口互联用户)
- **Role**: Must be `administrator`

**Required Password Policy**
Verify the following setting in **System Management** → **Rights Management** → **Rights Management Policy**:
- **Policy**: `Whether to modify the password of an interface interconnection user upon password resetting and first login`
- **Value**: Must be set to **`No`**

> **Impact:** If set to `Yes`, the user's password will be forced to change upon first login, breaking the authentication and causing cluster creation failures.

### 5. Public Registry Configuration
Configure the public registry credentials on the <Term name="product" textCase="capitalize" />. This includes:
- Registry repository address configuration
- Proper authentication credentials setup

For detailed configuration steps, refer to the **Alauda Container Platform** documentation: **Configure → Clusters → How to → Updating Public Registry Credentials**.

## Cluster Creation Overview

At a high level, you'll create the following Cluster API resources in the <Term name="product" textCase="capitalize" />'s `global` cluster to provision infrastructure and bootstrap a functional Kubernetes cluster.



:::warning
**Important Namespace Requirement**

To ensure proper integration with the <Term name="product" textCase="capitalize" /> as business clusters, all resources must be deployed in the `cpaas-system` namespace. Deploying resources in other namespaces may result in integration issues.
:::

## Control Plane Configuration

The control plane manages cluster state, scheduling, and the Kubernetes API. This section shows how to configure a highly available control plane.

:::warning
**Configuration Parameter Guidelines**

When configuring resources, exercise caution with parameter modifications:
- **Replace only values enclosed in `<>`** with your environment-specific values
- **Preserve all other parameters** as they represent optimized or required configurations
- Modifying non-placeholder parameters may result in cluster instability or integration issues
:::

### Configuration Workflow

Follow these steps in order:

1. Plan network and deploy the API load balancer
2. Configure DCS credentials (Secret)
3. Create IP and hostname pool
4. Create the control plane `DCSMachineTemplate`
5. Configure `KubeadmControlPlane`
6. Configure `DCSCluster`
7. Create the `Cluster`

After applying the manifests, a DCS kubernetes control plane is created by <Term name="product" textCase="capitalize" />.

### Network Planning and Load Balancer

Before creating control plane resources, plan the network architecture and deploy a load balancer for high availability.

#### Requirements

- **Network segmentation**: Plan IP address ranges for control plane nodes
- **Load balancer**: Deploy and configure access to the API server
- **IP association**: Bind the load balancer to an IP from the control plane IP pool
- **Connectivity**: Ensure network connectivity between all components

The load balancer distributes API server traffic across control plane nodes to ensure availability and fault tolerance.

### Configure DCS Authentication

DCS authentication information is stored in a Secret resource.

In the following example, `<auth-secret-name>` is the name of the saved Secret:

```yaml
apiVersion: v1
data:
  authUser: <base64-encoded-auth-user>
  authKey: <base64-encoded-auth-key>
  endpoint: <base64-encoded-endpoint>
kind: Secret
metadata:
  name: <auth-secret-name>
  namespace: cpaas-system
type: Opaque
```

| Parameter | Description |
| --- | --- |
| .data.authUser | DCS platform API user login name (base64-encoded) |
| .data.authKey | DCS platform API user login password (base64-encoded) |
| .data.endpoint | DCS platform API address with http or https protocol (base64-encoded). **Note**: The default API port for DCS platform is 7443 (not the common 8443). If your environment uses a custom port, confirm with your administrator. |

:::info
Before configuring this Secret, ensure that your DCS platform credentials meet the requirements specified in [Prerequisites → DCS Platform Credential Requirements](#5-dcs-platform-credential-requirements).
:::

### Configure IP and Hostname Pool

You need to plan the control plane virtual machines' IP addresses, hostnames, DNS servers, and other network information in advance.

:::warning
You must configure machine information for a number of machines greater than or equal to the number of control plane nodes.
:::

In the following example, `<control-plane-iphostname-pool-name>` is the resource name:

```yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DCSIpHostnamePool
metadata:
  name: <control-plane-iphostname-pool-name>
  namespace: cpaas-system
spec:
  pool:
  - ip: "<control-plane-ip-1>"
    mask: "<control-plane-mask>"
    gateway: "<control-plane-gateway>"
    dns: "<control-plane-dns>"
    hostname: "<control-plane-hostname-1>"
    machineName: "<control-plane-machine-name-1>"
```

| Parameter | Description | Required |
| --- | --- | --- |
| .spec.pool[].ip | IP address for the virtual machine to be created | Yes |
| .spec.pool[].mask | Subnet mask | Yes |
| .spec.pool[].gateway | Gateway IP address | Yes |
| .spec.pool[].dns | DNS server IP (use ',' to separate multiple servers) | No |
| .spec.pool[].machineName | Name of the virtual machine in the DCS platform | No |
| .spec.pool[].hostname | Hostname of the virtual machine | No |

### Configure Machine Template (Control Plane)

The DCS machine template declares the configuration for DCS machines created by subsequent Cluster API components. The machine template specifies the virtual machine template, attached disks, CPU, memory, and other configuration information.

:::warning
**Storage Requirements**

**Datastore Cross-Host Access**
The datastore clusters (`datastoreClusterName`) must support cross-host access across *all* physical machines in the DCS platform.
*   *Why*: If a datastore is only available on specific hosts, VM creation will fail when the DCS platform attempts to schedule the VM on a different host.

**Shared Storage for Ignition**
If your datastore does not support direct file uploads (required for Ignition configs), you **must** provide a shared storage solution (e.g., NFS) that supports multi-host mounting.

**Disk Configuration Rules**
You may add custom disks, but **must retain** the mandatory system and data disks shown in the example below (`systemVolume`, `/var/lib/kubelet`, `/var/lib/containerd`, `/var/cpaas`).
:::

In the following example, `<cp-dcs-machine-template-name>` is the control plane machine template name:

```yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DCSMachineTemplate
metadata:
  name: <cp-dcs-machine-template-name>
  namespace: cpaas-system
spec:
  template:
    spec:
      vmTemplateName: <vm-template-name>
      location:
        type: folder
        name: <folder-name>
      resource: # Optional, if not specified, uses template defaults
        type: cluster # cluster | host. Optional
        name: <cluster-name> # Optional
      vmConfig:
        dvSwitchName: <dv-switch-name> # Optional
        portGroupName: <port-group-name> # Optional
        dcsMachineCpuSpec:
          quantity: <control-plane-cpu>
        dcsMachineMemorySpec: # MB
          quantity: <control-plane-memory>
        dcsMachineDiskSpec: # GB
        - quantity: 0
          datastoreClusterName: <datastore-cluster-name>
          systemVolume: true
        - quantity: 10
          datastoreClusterName: <datastore-cluster-name>
          path: /var/lib/etcd
          format: xfs
        - quantity: 100
          datastoreClusterName: <datastore-cluster-name>
          path: /var/lib/kubelet
          format: xfs
        - quantity: 100
          datastoreClusterName: <datastore-cluster-name>
          path: /var/lib/containerd
          format: xfs
        - quantity: 100
          datastoreClusterName: <datastore-cluster-name>
          path: /var/cpaas
          format: xfs
      ipHostPoolRef:
        name: <control-plane-iphostname-pool-name>
```

#### Key Parameter Descriptions

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| .spec.template.spec.vmTemplateName | string | DCS virtual machine template name | Yes |
| .spec.template.spec.location | object | Location where the VM will be created (auto-selected if not specified) | No |
| .spec.template.spec.location.type | string | VM creation location type (currently only supports "folder") | Yes |
| .spec.template.spec.location.name | string | VM creation folder name | Yes |
| .spec.template.spec.resource | object | Compute resource selection for VM creation (auto-selected if not specified) | No |
| .spec.template.spec.resource.type | string | Compute resource type: cluster or host | Yes |
| .spec.template.spec.resource.name | string | Compute resource name | Yes |
| .spec.template.spec.vmConfig | object | Virtual machine configuration | Yes |
| .spec.template.spec.vmConfig.dvSwitchName | string | Virtual machine switch name (uses template default if not specified) | No |
| .spec.template.spec.vmConfig.portGroupName | string | Port group name (must belong to the above switch, uses template default if not specified) | No |
| .spec.template.spec.vmConfig.dcsMachineCpuSpec.quantity | int | VM CPU specification (cores) | Yes |
| .spec.template.spec.vmConfig.dcsMachineMemorySpec.quantity | int | VM memory size (MB) | Yes |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec | []object | VM disk configuration | Yes |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec[].quantity | int | Disk size (GB). For system disk, 0 auto-sets to template system disk size | Yes |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec[].datastoreClusterName | string | Datastore cluster name for the disk | Yes |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec[].systemVolume | bool | Whether this is the system disk (only one disk can be true) | No |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec[].path | string | Disk mount directory (disk won't be mounted if not specified) | No |
| .spec.template.spec.vmConfig.dcsMachineDiskSpec[].format | string | File system format | No |
| .spec.template.spec.ipHostPoolRef.name | string | Referenced DCSIpHostnamePool name | Yes |

### Configure KubeadmControlPlane

The current DCS control plane implementation relies on the Cluster API control plane provider kubeadm and requires configuring the KubeadmControlPlane resource.

Most parameters in the example are already optimized or required configurations, but some parameters may need customization based on your environment.

In the following example, `<kcp-name>` is the resource name:


```yaml
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: <kcp-name>
  namespace: cpaas-system
  annotations:
    controlplane.cluster.x-k8s.io/skip-kube-proxy: ""
spec:
  rolloutStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
  kubeadmConfigSpec:
    users:
    - name: boot
      sshAuthorizedKeys:
      - "<ssh-authorized-keys>"
    format: ignition
    files:
    - path: /etc/kubernetes/admission/psa-config.yaml
      owner: "root:root"
      permissions: "0644"
      content: |
        # ... (Admission Configuration Content) ...
    - path: /etc/kubernetes/patches/kubeletconfiguration0+strategic.json
      owner: "root:root"
      permissions: "0644"
      content: |
        {
          "apiVersion": "kubelet.config.k8s.io/v1beta1",
          "kind": "KubeletConfiguration",
          # ... (Kubelet Configuration Content) ...
        }
    - path: /etc/kubernetes/encryption-provider.conf
      owner: "root:root"
      append: false
      permissions: "0644"
      content: |
        apiVersion: apiserver.config.k8s.io/v1
        kind: EncryptionConfiguration
        resources:
        - resources:
          - secrets
          providers:
          - aescbc:
              keys:
              - name: key1
                secret: <base64-encoded-secret>
    - path: /etc/kubernetes/audit/policy.yaml
      owner: "root:root"
      append: false
      permissions: "0644"
      content: |
        # ... (Audit Policy Content) ...

    preKubeadmCommands:
    - while ! ip route | grep -q "default via"; do sleep 1; done; echo "NetworkManager started"
    - mkdir -p /run/cluster-api && restorecon -Rv /run/cluster-api
    - if [ -f /etc/disk-setup.sh ]; then bash /etc/disk-setup.sh; fi
    postKubeadmCommands:
    - chmod 600 /var/lib/kubelet/config.yaml
    clusterConfiguration:
      imageRepository: cloud.alauda.io/alauda
      dns:
        imageTag: <dns-image-tag>
      etcd:
        local:
          imageTag: <etcd-image-tag>
      apiServer:
        extraArgs:
          audit-log-format: json
          audit-log-maxage: "30"
          audit-log-maxbackup: "10"
          audit-log-maxsize: "200"
          profiling: "false"
          audit-log-mode: batch
          audit-log-path: /etc/kubernetes/audit/audit.log
          audit-policy-file: /etc/kubernetes/audit/policy.yaml
          tls-cipher-suites: "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384"
          encryption-provider-config: /etc/kubernetes/encryption-provider.conf
          admission-control-config-file: /etc/kubernetes/admission/psa-config.yaml
          tls-min-version: VersionTLS12
          kubelet-certificate-authority: /etc/kubernetes/pki/ca.crt
        extraVolumes:
        - name: vol-dir-0
          hostPath: /etc/kubernetes
          mountPath: /etc/kubernetes
          pathType: Directory
      controllerManager:
        extraArgs:
          bind-address: "::"
          profiling: "false"
          tls-min-version: VersionTLS12
          flex-volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/"
      scheduler:
        extraArgs:
          bind-address: "::"
          tls-min-version: VersionTLS12
          profiling: "false"
    initConfiguration:
      patches:
        directory: /etc/kubernetes/patches
      nodeRegistration:
        kubeletExtraArgs:
          node-labels: "kube-ovn/role=master"
          provider-id: PROVIDER_ID
          volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/"
          protect-kernel-defaults: "true"
    joinConfiguration:
      patches:
        directory: /etc/kubernetes/patches
      nodeRegistration:
        kubeletExtraArgs:
          node-labels: "kube-ovn/role=master"
          provider-id: PROVIDER_ID
          volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/"
          protect-kernel-defaults: "true"
  machineTemplate:
    nodeDrainTimeout: 1m
    nodeDeletionTimeout: 5m
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DCSMachineTemplate
      name: <cp-dcs-machine-template-name>
  replicas: 3
  version: <control-plane-kubernetes-version>
```

#### Key Parameter Descriptions

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| .spec.kubeadmConfigSpec | object | kubeadm bootstrap provider startup parameters for customizing VM startup configuration (users, network, files, etc.) | Yes |
| .spec.kubeadmConfigSpec.users | []object | User configuration | No |
| .spec.machineTemplate.infrastructureRef | string | DCSMachineTemplate name for creating DCSMachine resources | Yes |
| .spec.replicas | int | Control plane VM replica count (cannot exceed the configuration count in the referenced IpHostnamePool) | Yes |
| .spec.version | string | Control plane Kubernetes version (must match VM template version) | Yes |

### Configure DCSCluster

DCSCluster is the infrastructure cluster declaration. Since the DCS platform currently doesn't provide a native load balancer, you need to manually configure the load balancer in advance and bind it to an IP address from the IP-hostname pool configured in the "Configure Virtual Machine IP and Hostname Pool" section.

In the following example, `<dcs-cluster-name>` is the resource name:

```yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DCSCluster
metadata:
  name: "<dcs-cluster-name>"
  namespace: cpaas-system
spec:
  controlPlaneLoadBalancer: # Configure HA
    host: <load-balancer-ip-or-domain-name>
    port: 6443
    type: external 
  credentialSecretRef: # Reference authentication secret
    name: <auth-secret-name>
  controlPlaneEndpoint: # Cluster API specification, keep consistent with controlPlane
    host: <load-balancer-ip-or-domain-name>
    port: 6443
  networkType: kube-ovn 
  site: <site>  # DCS platform parameter, resource pool ID
```

#### Key Parameter Descriptions

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| .spec.controlPlaneLoadBalancer | object | Control plane API server exposure method | Yes |
| .spec.controlPlaneLoadBalancer.type | string | Currently only supports "external" | Yes |
| .spec.controlPlaneLoadBalancer.host | string | Load balancer IP or domain name | Yes |
| .spec.controlPlaneLoadBalancer.port | int64 | Port number | Yes |
| .spec.credentialSecretRef.name | string | DCS cluster authentication information (see "Configure DCS Authentication Information" section) | Yes |
| .spec.controlPlaneEndpoint | object | API server exposure address (Cluster API specification) | No |
| .spec.networkType | string | Currently only supports "kube-ovn" | Yes |
| .spec.site | string | DCS platform site ID | Yes |

### Configure Cluster

The Cluster resource in Cluster API is used to declare a cluster and needs to reference the corresponding control plane resource and infrastructure cluster resource:

```yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  annotations:
    capi.cpaas.io/resource-group-version: infrastructure.cluster.x-k8s.io/v1beta1
    capi.cpaas.io/resource-kind: DCSCluster
    cpaas.io/kube-ovn-version: <kube-ovn-version>
    cpaas.io/kube-ovn-join-cidr: <kube-ovn-join-cidr>
  labels:
    cluster-type: DCS
  name: <cluster-name>
  namespace: cpaas-system
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - <pods-cidr>
    services:
      cidrBlocks:
      - <services-cidr>
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: <kubeadm-control-plane-name>
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DCSCluster
    name: <dcs-cluster-name-for-nodes>
```

#### Key Parameter Descriptions

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| .spec.clusterNetwork.pods.cidrBlocks | []string | Pod CIDR | No |
| .spec.clusterNetwork.services.cidrBlocks | []string | Service network CIDR | No |
| .spec.controlPlaneRef | object | Control plane reference (see "Configure KubeadmControlPlane Resource" section) | Yes |
| .spec.infrastructureRef | object | Infrastructure cluster reference (see "Configure DCSCluster Resource" section) | Yes |

## Deploying Nodes

Refer to the [Deploy Nodes](./node.mdx) page for instructions.

## Cluster Verification

After deploying all cluster resources, verify that the cluster has been created successfully and is operational.

### Using the <Term name="productShort" textCase="upper" /> Console

1. Navigate to the **Administrator** view in the <Term name="productShort" textCase="upper" /> console
2. Go to **Clusters** → **Clusters**
3. Locate your newly created cluster in the cluster list
4. Verify that the cluster status shows as **Running**
5. Check that all control plane and worker nodes are **Ready**

### Using kubectl

Alternatively, you can verify the cluster using kubectl commands:

```bash
# Check cluster status
kubectl get cluster -n cpaas-system <cluster-name>

# Verify control plane nodes
kubectl get kubeadmcontrolplane -n cpaas-system <kcp-name>

# Check machine status
kubectl get machines -n cpaas-system

# Verify cluster deployment status
kubectl get clustermodule <cluster-name> -o jsonpath='{.status.base.deployStatus}'
```

### Expected Results

A successfully created cluster should show:
- Cluster status: **Running** or **Provisioned**
- All control plane machines: **Running**
- All worker nodes (if deployed): **Running**
- Kubernetes nodes: **Ready**
- Cluster Module Status: **Completed**




